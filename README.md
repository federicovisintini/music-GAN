# Generative Adversarial Network for Music Generation

This project is a Generative Adversarial Network (GAN) designed for generating music. The main entry points for the project are `src/train.py` and `src/infer.py`.

Several utilities to convert data to audio and to images are available in `src/music_player/` and `src/visualize/`.

## Model Description

The model is inspired by Convolutional Neural Networks (CNNs), where one dimension represents the note (1-hot-encoded) and the other represents time. Multiple tracks are processed in parallel. The parameters of the models can be found in `src/train.py` for training-related parameters and in `src/config.py` for the model structure.

Ideas for different architectures are located in `src/gan/dev.py` but still need to be optimized. Several regularization ideas have been tried to make GAN training converge.

## Samples

Here are some samples generated by the model (16 notes x 2 tracks):
- [Sample After 1 Training Epoch.mid](data%2Ftrain_song_epoch_0.mid)
- [Sample After 2 Training Epoch.mid](data%2Ftrain_song_epoch_1.mid)
- [Sample After 3 Training Epoch.mid](data%2Ftrain_song_epoch_2.mid)
- [Sample After 4 Training Epoch.mid](data%2Ftrain_song_epoch_3.mid)

## Visualization
This is example of how we handle data and regularize the input for the discriminator
![real_song_with_noise.png](data%2Freal_song_with_noise.png)
